{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "general-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "material-kelly",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "task1='sentiment'\n",
    "MODEL1 = f\"cardiffnlp/twitter-roberta-base-{task1}\"\n",
    "model1 = TFAutoModelForSequenceClassification.from_pretrained(MODEL1)\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(MODEL1)\n",
    "\n",
    "task2='emotion'\n",
    "MODEL2 = f\"cardiffnlp/twitter-roberta-base-{task2}\"\n",
    "model2 = TFAutoModelForSequenceClassification.from_pretrained(MODEL2)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(MODEL2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "textile-insulin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'neutral', 'positive']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download label mapping emotion\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task1}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels1 = [row[1] for row in csvreader if len(row) > 1]\n",
    "labels1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unauthorized-healthcare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'joy', 'optimism', 'sadness']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download label mapping emotion\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task2}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels2 = [row[1] for row in csvreader if len(row) > 1]\n",
    "labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "expensive-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PT\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "#text = \"Good night üòä\"\n",
    "#text = preprocess(text)\n",
    "#encoded_input = tokenizer(text, return_tensors='pt')\n",
    "#output = model(**encoded_input)\n",
    "#scores = output[0][0].detach().numpy()\n",
    "#scores = softmax(scores)\n",
    "\n",
    "\n",
    "# # TF\n",
    "text = \"mean hate bts feels like home still respect like bts alot effect life depression specific eternal musician artists adore course whole family armys #btswins\"\n",
    "encoded_input = tokenizer1(text, return_tensors='tf')\n",
    "output = model1(encoded_input)\n",
    "scores = output[0][0].numpy()\n",
    "scores = softmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "demographic-operation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) neutral 0.5262\n",
      "2) positive 0.2854\n",
      "3) negative 0.1885\n"
     ]
    }
   ],
   "source": [
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels1[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "centered-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TF\n",
    "text = \"doesn't mean hate bts feels like home still respect like bts alot effect life depression specific eternal musician artists adore course whole family armys #btswins\"\n",
    "encoded_input = tokenizer1(text, return_tensors='tf')\n",
    "output = model1(encoded_input)\n",
    "scores = output[0][0].numpy()\n",
    "scores = softmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "architectural-annual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) positive 0.6778\n",
      "2) neutral 0.2981\n",
      "3) negative 0.0241\n"
     ]
    }
   ],
   "source": [
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels1[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "constitutional-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # TF\n",
    "text = \"It doesn't mean we hate 1D no but BTS feels like home still we respect 1D and we like them.BTS had alot of effect on my life, depression to be specific that is why they are my eternal musician and artists I ADORE THEM and of course my whole family ARMYsüå∏üçÉ #BTSWins10s @BTS_twt\"\n",
    "encoded_input = tokenizer1(text, return_tensors='tf')\n",
    "output = model2(encoded_input)\n",
    "scores = output[0][0].numpy()\n",
    "scores = softmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "whole-network",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) joy 0.9211\n",
      "2) optimism 0.0427\n",
      "3) sadness 0.02\n",
      "4) anger 0.0162\n"
     ]
    }
   ],
   "source": [
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels2[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-friendship",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
